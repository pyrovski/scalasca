/****************************************************************************
**  SCALASCA    http://www.scalasca.org/                                   **
*****************************************************************************
**  Copyright (c) 1998-2013                                                **
**  Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre          **
**                                                                         **
**  See the file COPYRIGHT in the package base directory for details       **
****************************************************************************/


/*
 *---------------------------------------------------------------------------
 *
 * PEARL.hybrid
 *
 *---------------------------------------------------------------------------
 */


//--- Library description ---------------------------------------------------

/**
@defgroup PEARL_hybrid PEARL.hybrid
@ingroup  PEARL
@brief    Hybrid OpenMP/MPI-related part of the PEARL library

This part of the PEARL library provides all functions and classes that are
specific to the handling traces of hybrid OpenMP/MPI applications.

The following code snippet shows the basic steps required to load and set up
the PEARL data structures to handle hybrid OpenMP/MPI traces (for information
on how to handle serial, pure OpenMP or pure MPI traces, see the @ref
PEARL_base, @ref PEARL_omp, and @ref PEARL_mpi parts of PEARL).

@code
  // Initialize MPI, etc.
  ...
  // Initialize PEARL
  PEARL_hybrid_init();

  // Load global definitions
  GlobalDefs defs(archive_name);

  // Create thread team
  #pragma omp parallel
  {
    // Load trace data
    LocalTrace trace(archive_name, mpi_rank, omp_get_thread_num());

    // Preprocessing
    PEARL_omp_verify_calltree(defs, trace);
    #pragma omp master
    {
      PEARL_mpi_unify_calltree(defs);
    }
    PEARL_omp_preprocess_trace(defs, trace);
    ...
  }
@endcode

Note that all of the aforementioned function calls except PEARL_hybrid_init()
throw exceptions in case of errors. This has to be taken into account to
avoid deadlocks (e.g., one process failing with an exception while the other
processes wait in an MPI communication operation or OpenMP barrier).
*/
